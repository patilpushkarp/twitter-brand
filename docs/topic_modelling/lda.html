
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Topic Modeling: Latent Dirichlet Allocation (LDA) &#8212; Understanding Brands</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Topic Modeling: BERTopic" href="bertopic.html" />
    <link rel="prev" title="Topic Modeling" href="topic_modeling.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Understanding Brands</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Social Media Analytics
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../eda/eda.html">
   Exploratory Data Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../eda/preprocess.html">
   Data Preprocessing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="topic_modeling.html">
   Topic Modeling
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Topic Modeling: Latent Dirichlet Allocation (LDA)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bertopic.html">
   Topic Modeling: BERTopic
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="top2vec.html">
   Topic Modeling: Top2Vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sentiment_analysis/sentiment_analysis_comp.html">
   Sentiment Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../code/scrappers/starbucks.html">
   Scrapper for Starbucks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../team.html">
   Project by
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/patilpushkarp/twitter-brand/master?urlpath=tree/docs/docs/topic_modelling/lda.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/patilpushkarp/twitter-brand"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/patilpushkarp/twitter-brand/issues/new?title=Issue%20on%20page%20%2Fdocs/topic_modelling/lda.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/docs/topic_modelling/lda.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lda-with-bag-of-words">
   LDA with bag-of-words
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lda-with-tf-idf">
   LDA with TF-IDF
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Topic Modeling: Latent Dirichlet Allocation (LDA)</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lda-with-bag-of-words">
   LDA with bag-of-words
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lda-with-tf-idf">
   LDA with TF-IDF
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="topic-modeling-latent-dirichlet-allocation-lda">
<h1>Topic Modeling: Latent Dirichlet Allocation (LDA)<a class="headerlink" href="#topic-modeling-latent-dirichlet-allocation-lda" title="Permalink to this headline">#</a></h1>
<p>One of the widely used algorithms for topic modeling is Latent Dirichlet Allocation (LDA). Latent Dirichlet Allocation (LDA) is a statistical model that is used to discover the underlying topics in a collection of documents. It is a generative model that assumes that each document is a mixture of a fixed number of topics, and each topic is a mixture of a fixed number of words.</p>
<p>In LDA, each document is represented as a distribution over the set of topics, and each topic is represented as a distribution over the vocabulary of words. Given a set of documents, LDA estimates the topic distributions for each document and the word distributions for each topic.</p>
<p>LDA is used in a wide range of applications, including information retrieval, document classification, and topic modeling. It is a popular technique for discovering the underlying themes in a large corpus of text data and has been widely used in natural language processing and text mining.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Load necessary packages
import re
import gensim
import numpy as np
import pandas as pd
import swifter
import pyLDAvis
import pyLDAvis.gensim_models

import plotly.express as px
import plotly.io as pio
svg_renderer = pio.renderers[&quot;svg&quot;]
svg_renderer.width = 900
svg_renderer.height = 500
pio.renderers.default = &quot;svg&quot;
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import seaborn as sns
from wordcloud import WordCloud

from tqdm import tqdm

# pyLDAvis.enable_notebook()
tqdm.pandas()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses
  from imp import reload
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Load the preprocessed data
df = pd.read_hdf(&#39;./../../code/data/starbucks/data.h5&#39;, key=&#39;preprocessed_starbucks&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Confirm the shape
df.shape
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(2770, 29)
</pre></div>
</div>
</div>
</div>
<p>To improve the results, words coming together for large number of documents can be clubbed and then fed to the model. For the analysis, bigrams and trigrams will be used.</p>
<p><strong>Bigrams</strong></p>
<p>A bigram is a sequence of two adjacent words in a string of text. In natural language processing, bigrams are often used as a basic unit for measuring the similarity of two texts or for identifying the key topics in a document.</p>
<p>For example, consider the following sentence: “I went to the store to buy some milk.”</p>
<p>The bigrams in this sentence would be: “I went”, “went to”, “to the”, “the store”, “store to”, “to buy”, “buy some”, “some milk”.</p>
<p>Bigrams can be useful for identifying common patterns or trends in a text. For example, if we counted the number of times each bigram occurred in a large collection of texts, we might find that certain bigrams are much more common than others, which could help us identify the most important topics or themes in the texts. Bigrams can also be used as features in machine learning models for tasks such as text classification or sentiment analysis.</p>
<p><strong>Trigrams</strong></p>
<p>Trigrams are groups of three consecutive characters or words in a given text. They are commonly used in natural language processing tasks, such as language modeling, machine translation, and spelling correction, as a way of capturing the context and relationships between words in a language.</p>
<p>For example, in the sentence “The cat sat on the mat,” the trigrams would be “The cat,” “cat sat,” “sat on,” “on the,” and “the mat.” Trigrams can provide valuable information about the likelihood of certain words or phrases appearing together, which can be useful for tasks such as language generation or prediction.</p>
<p>Trigrams can be computed from a given text by splitting the text into a list of three-word chunks, or by generating a list of all possible three-word combinations. They can also be computed from a list of words, by generating all possible combinations of three consecutive words in the list.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Create Bigrams
def sentence_to_words(sentences):
    &quot;&quot;&quot;Function to convert sentences to words&quot;&quot;&quot;
    return (gensim.utils.simple_preprocess(str(sentence), deacc=True) for sentence in sentences)

data = df[&#39;preprocessed_tweet&#39;].values.tolist()
data_words = list(sentence_to_words(data))

bigram = gensim.models.phrases.Phrases(data_words, min_count=5, threshold=10, connector_words=gensim.models.phrases.ENGLISH_CONNECTOR_WORDS)
trigram = gensim.models.Phrases(bigram[data_words], threshold=10, connector_words=gensim.models.phrases.ENGLISH_CONNECTOR_WORDS)

bigram_model = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)

df.loc[:, &#39;sep_words&#39;] = df[&#39;preprocessed_tweet&#39;].swifter.apply(lambda x: list(sentence_to_words([x]))[0])
df[&#39;bigram&#39;] = df[&#39;sep_words&#39;].swifter.apply(lambda x: bigram_model[x])
df[&#39;trigram&#39;] = df[&#39;bigram&#39;].swifter.apply(lambda x: trigram_mod[x])
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "4852b9f3abd74447b46280c2241b8a16", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "a214af0b632c4cd2bc556dddc921ee47", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "bea928d34b2f47a5869a8d4481a24b06", "version_major": 2, "version_minor": 0}
</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Copy the data to another dataframe
main_df = df.copy()
</pre></div>
</div>
</div>
</div>
<section id="lda-with-bag-of-words">
<h2>LDA with bag-of-words<a class="headerlink" href="#lda-with-bag-of-words" title="Permalink to this headline">#</a></h2>
<p>As such words can not be interpreted by the machine. Hence, it is necessary to convert words into appropriate numbers. One of the simplest ways to do such a conversion is by using the bag-of-words model.</p>
<p>The bag-of-words model is a simple and effective representation for text data. It is a way of representing text data as numerical feature vectors. The bag-of-words model represents each document as a fixed-length vector, where each dimension of the vector represents a particular word in the vocabulary.</p>
<p>In the bag-of-words model, the order of the words in the document is not important. Instead, the model only looks at the presence or absence of each word in the vocabulary. If a word is present in the document, the corresponding dimension in the feature vector is set to a non-zero value, such as 1. If a word is not present in the document, the corresponding dimension is set to zero.</p>
<p>The bag-of-words model is a simple and effective way to represent text data for many natural language processing tasks, such as text classification, information retrieval, and language modeling. It is also a good starting point for more advanced models that take into account the order of the words in a document, such as n-gram models or recurrent neural networks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Create the dictionary of words for LDA
id2word = gensim.corpora.Dictionary(df[&#39;trigram&#39;].values.tolist())
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Convert text to numbers using bag-of-words model
bow_corpus = [id2word.doc2bow(text) for text in df[&#39;trigram&#39;].values.tolist()]
</pre></div>
</div>
</div>
</div>
<p>For the current data, the number of topics is unknown. Hence, it is necessary to use coherence metric to find the optimum number of topics in the data set.</p>
<p>Coherence is a measure of how interpretable or meaningful the topics generated by a topic model are. In Latent Dirichlet Allocation (LDA), coherence measures the degree to which the words in a topic tend to occur together in the same documents. A topic with high coherence is one in which the words are more likely to appear together in the same documents and form a coherent concept or theme.</p>
<p>There are several methods for calculating coherence in LDA, but the most common method is to use a measure called “topic coherence”. This measure is based on the idea that the words in a coherent topic should have high mutual information, which means that the presence of one word in a document is a strong predictor of the presence of the other words in the same topic.</p>
<p>To calculate topic coherence, we first need to compute the pairwise mutual information between all pairs of words in a given topic. Then, we sum the mutual information values for all pairs of words and divide by the total number of pairs to get the average mutual information. The higher the average mutual information, the higher the coherence of the topic.</p>
<p>There are various approaches to calculating mutual information, such as using pointwise mutual information or normalized pointwise mutual information. These measures can be computed using the frequency of co-occurrence of words in a document or by using a statistical model such as LDA to estimate the probability of co-occurrence.</p>
<p>In this analysis, UMass will be used as a coherence measure.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>## Find the appropriate number of topics

# Declare the range for the number of topics to be searched for
topics_range = range(3, 20, 1)

# Initialize a blank variable object for storing the results
model_results = {
    &#39;Number of topics&#39;: [],
    &#39;Coherence Score&#39;: []
}

# Interate over the range
for k in tqdm(topics_range):

    # Build the LDA model
    lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,
                                            id2word=id2word,
                                            num_topics=k,
                                            random_state=42,
                                            chunksize=500,
                                            passes=10,
                                            alpha=&#39;asymmetric&#39;,
                                            eta=&#39;auto&#39;,
                                            per_word_topics=True)
    
    # Build the coherence model from the LDA model
    coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, 
                                                    texts=df[&#39;bigram&#39;].values.tolist(), 
                                                    dictionary=id2word,
                                                    coherence=&#39;u_mass&#39;)
    # Get the coherence score
    coherence_score = coherence_model_lda.get_coherence()

    # Append the values to the results object
    model_results[&#39;Number of topics&#39;].append(k)
    model_results[&#39;Coherence Score&#39;].append(coherence_score)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████| 17/17 [02:24&lt;00:00,  8.47s/it]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Convert the result dictionary to dataframe
model_results = pd.DataFrame(model_results)

# Get the best k and the corresponding coherence score
best_coherence = model_results[&#39;Coherence Score&#39;].max()
best_k_index = model_results[model_results[&#39;Coherence Score&#39;] == best_coherence].index
best_k = model_results.iloc[best_k_index][&quot;Number of topics&quot;].values[0]

model_results
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Number of topics</th>
      <th>Coherence Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>-5.488991</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4</td>
      <td>-5.851549</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5</td>
      <td>-6.113736</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6</td>
      <td>-7.343848</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7</td>
      <td>-7.767120</td>
    </tr>
    <tr>
      <th>5</th>
      <td>8</td>
      <td>-7.788252</td>
    </tr>
    <tr>
      <th>6</th>
      <td>9</td>
      <td>-8.851397</td>
    </tr>
    <tr>
      <th>7</th>
      <td>10</td>
      <td>-8.929948</td>
    </tr>
    <tr>
      <th>8</th>
      <td>11</td>
      <td>-8.928888</td>
    </tr>
    <tr>
      <th>9</th>
      <td>12</td>
      <td>-8.959752</td>
    </tr>
    <tr>
      <th>10</th>
      <td>13</td>
      <td>-9.148446</td>
    </tr>
    <tr>
      <th>11</th>
      <td>14</td>
      <td>-9.953639</td>
    </tr>
    <tr>
      <th>12</th>
      <td>15</td>
      <td>-9.460240</td>
    </tr>
    <tr>
      <th>13</th>
      <td>16</td>
      <td>-9.575802</td>
    </tr>
    <tr>
      <th>14</th>
      <td>17</td>
      <td>-10.190708</td>
    </tr>
    <tr>
      <th>15</th>
      <td>18</td>
      <td>-10.093989</td>
    </tr>
    <tr>
      <th>16</th>
      <td>19</td>
      <td>-10.786797</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Plot the reuslts
fig = px.line(model_results, x=&quot;Number of topics&quot;, y=&quot;Coherence Score&quot;, title=&#39;Scree Plot for LDA using bag-of-words&#39;, markers=True)
fig.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/plotly/io/_renderers.py:395: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/plotly/io/_renderers.py:395: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.
</pre></div>
</div>
<img alt="../../_images/lda_15_1.svg" src="../../_images/lda_15_1.svg" /></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(f&quot;From the above graph, it can be observed that the best coherence score of {best_coherence} is obtained using {best_k} topics is closest to 0. Hence, for this data set the optimum number of topics is {best_k}.&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>From the above graph, it can be observed that the best coherence score of -5.488990997220976 is obtained using 3 topics is closest to 0. Hence, for this data set the optimum number of topics is 3.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Create final models with optimum k i.e. 5 for further analysis
lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,
                                            id2word=id2word,
                                            num_topics=best_k,
                                            random_state=42,
                                            chunksize=500,
                                            passes=10,
                                            alpha=&#39;asymmetric&#39;,
                                            eta=&#39;auto&#39;,
                                            per_word_topics=True)
    
coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, 
                                                corpus=bow_corpus, 
                                                dictionary=id2word, 
                                                coherence=&#39;u_mass&#39;)

coherence_score = coherence_model_lda.get_coherence()
print(f&quot;The coherence for the {best_k} number of topics is: {coherence_score}&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The coherence for the 3 number of topics is: -5.522843429085495
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Plot the distribution of words in each topic
LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, bow_corpus, id2word)
pyLDAvis.display(LDAvis_prepared)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses
  from imp import reload
/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses
  from imp import reload
/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses
  from imp import reload
/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses
  from imp import reload
/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses
  from imp import reload
/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses
  from imp import reload
/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses
  from imp import reload
/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses
  from imp import reload
</pre></div>
</div>
<div class="output text_html">
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css">


<div id="ldavis_el555510858257424942357348"></div>
<script type="text/javascript">

var ldavis_el555510858257424942357348_data = {"mdsDat": {"x": [-0.10139751151592791, 0.036279006313612513, 0.06511850520231541], "y": [0.01407584395939444, -0.0812723368429973, 0.06719649288360281], "topics": [1, 2, 3], "cluster": [1, 1, 1], "Freq": [70.68215893006612, 19.369814731625016, 9.948026338308848]}, "tinfo": {"Term": ["work", "like", "want", "union", "company", "look", "worker", "job", "store", "barista", "think", "time", "employee", "cup", "support", "hour", "guy", "drink", "right", "hard", "reason", "eat", "ceo", "order", "amazon", "nft", "serve", "help", "real", "set", "morning", "order", "espresso", "pumpkin_spice", "offer", "taste", "nice", "ill", "drink", "bitch", "maybe", "mocha", "odyssey", "feel_like", "good_morning", "walk", "bean", "black", "fucking", "case", "early", "ice", "yesterday", "hand", "person", "imagine", "different", "going", "good_luck_wonderful_wednesday", "thank_support_entry_appreciate", "love", "know", "lol", "buy", "live", "fuck", "read", "wan_na", "try", "like", "new", "think", "people", "bad", "day", "today", "money", "iced", "need", "good", "tell", "come", "pay", "drive", "man", "stop", "use", "cup", "time", "want", "year", "work", "start", "barista", "free", "wage", "earn_complete_survey_point", "card_paypal", "invite_online_pay_survey", "redeem_amazon_itunes_gift", "gc", "contract", "office", "clothe", "circle", "pod", "verismo", "booth", "meeting", "election", "freediscounte", "xmas", "victory", "accuse", "mask", "support_entry", "bye", "usual_good_morning_thank", "appreciate_good_luck_wonderful", "dutch", "minimum", "svg", "nut", "report", "simply", "hard", "set", "raise", "loaf", "union", "fair", "pass", "seattle", "foam", "worker", "favourite", "elon", "bro", "wednesday", "eat", "break", "want", "book", "work", "hour", "store", "end", "time", "cup", "close", "huge", "enter", "job", "food", "barista", "thing", "year", "pay", "talk", "couple", "good", "start", "use", "week", "day", "need", "ask", "stop", "employee", "invest", "nv", "ab", "editing", "trust", "debate", "indians", "hamilton", "galaxy", "nolan", "midtown", "yoga", "alta", "impossible", "overtime", "decatur", "district", "schultz", "hilarious", "sob", "shareholder", "collar", "vote", "fundamental", "zayn", "diversity", "encourage", "rhode", "dick", "cody", "earning", "similar", "organize", "company", "support", "table", "look", "policy", "public", "progressive", "work", "job", "september", "supervisor", "training", "store", "link_bio", "child", "like", "instagram", "barista", "ceo", "unionized", "employee", "reason", "union", "worker", "serve", "help", "think", "right", "watch", "time", "amazon", "guy", "real", "big", "way", "people", "place"], "Freq": [136.0, 206.0, 104.0, 47.0, 38.0, 42.0, 54.0, 56.0, 62.0, 70.0, 87.0, 91.0, 34.0, 83.0, 13.0, 32.0, 33.0, 180.0, 40.0, 12.0, 16.0, 20.0, 17.0, 99.0, 20.0, 18.0, 14.0, 19.0, 20.0, 10.0, 60.2275989653926, 97.1665466718228, 19.8500888625333, 39.56674904055562, 20.563067702029322, 23.85477157064313, 20.47063129101713, 24.55760444627369, 175.33761343614484, 14.706063687059073, 15.490742214484372, 14.67116296031392, 13.020798585428652, 13.005320864639435, 13.809534010237247, 35.00435910162352, 12.158953024976102, 17.054604641250727, 20.216808575377073, 11.339685458254907, 12.94349348395571, 20.234735977630873, 12.107071532925158, 12.109317005371262, 19.307866793314687, 13.676447603190432, 15.279535523405022, 10.449292799830515, 9.640184425089336, 9.63908318455244, 81.95572293597142, 83.93776409472382, 49.1079367365695, 60.89805933767278, 26.389819019006993, 25.652665800157482, 20.868291563810462, 18.420083380486727, 60.6842268658538, 185.35645512070874, 70.39072630949043, 80.01552891762108, 85.4382357430172, 30.45349746452812, 93.56832093268692, 70.79153156651977, 34.841008421983894, 28.818370224098558, 75.71873306277074, 90.52982301887192, 47.108771783522656, 46.96835865452541, 74.99793310054271, 35.97602694940237, 29.336608068110444, 49.03008266834243, 49.168266860036766, 64.95092214602899, 67.18908605884009, 72.9929610052052, 48.73747016027737, 86.38300752687239, 43.4555303159429, 48.15037997999466, 37.5181340882649, 5.937513863185629, 4.5349742255208385, 4.533829617019749, 4.531989452679716, 4.5301492883396834, 4.485842154833547, 3.815388405016818, 3.8015416328171807, 3.799975386069748, 3.795087010571035, 3.7737495324602714, 3.112241718246374, 3.095956648998296, 4.3502645013328, 3.0750985833159383, 3.0789216482105104, 3.0666268491368553, 3.056589774472019, 3.0567433858266786, 3.0543363548061775, 4.292132799038172, 4.285820686441113, 4.2860907524215754, 4.279784077394593, 3.6217066902026653, 3.0022401287309703, 2.386520959303618, 2.385503227437645, 2.3837903928635686, 2.3829110924691257, 10.195294360135094, 8.335649925303347, 6.376483911016842, 3.5783756005252485, 29.661855953298367, 7.4588499068336125, 4.580269184257527, 7.108615111948873, 4.561995776884833, 26.98725660952288, 4.02847680743376, 4.019865508956049, 6.352226910905873, 4.835107702852163, 10.48333604100895, 9.012415334586295, 30.76457160199535, 7.757473099076208, 30.69496707997029, 12.399861990148587, 16.157516541748034, 8.241656088962124, 19.256317039896306, 18.12497439727026, 10.025305044405641, 6.63105961372445, 6.718886792468159, 13.505087077435842, 7.468377889000043, 13.914120130080414, 11.983124806967389, 12.297652891683551, 12.417995380091812, 8.01715060190803, 6.7026860055562665, 12.724803021561119, 10.185890082687903, 9.982388116315713, 9.031594550507554, 11.027102111269063, 10.489311024261271, 8.717121747753836, 8.584514820038182, 7.859849855951933, 2.9417018008592373, 1.9048154953612555, 1.8926884178418277, 1.8759135581562276, 2.7811938442423827, 1.840236305822398, 1.3392022931232217, 1.33716459036444, 1.3371318931011225, 1.335390327478082, 1.3336885571007875, 1.3332654708039093, 1.3326456191252873, 1.332441406680012, 1.3313312124119938, 1.3313797346853156, 1.3308854358434183, 1.330227650684933, 1.3303891588752705, 1.3293137563571882, 1.328849478490152, 1.3284408208788658, 3.4995529520505575, 1.3257289261497565, 1.3255386769486026, 1.3253929937682694, 1.3245799838793015, 1.3246046522772492, 1.3241855222328722, 1.3240736999195333, 1.7484058672543967, 2.4614464917371017, 3.0200367639839145, 13.426145143612393, 5.851133148229529, 3.325349612878708, 12.199227463568759, 2.4726943503183527, 2.4132297807407928, 1.9037912914049515, 19.446075761052175, 11.13267391872753, 2.993579209022704, 1.8387894810106866, 1.858173605581104, 9.905684560697408, 1.8178668396588276, 2.383948159656031, 15.680691498301526, 1.79849074395378, 8.095355545656776, 4.018588611791766, 2.871658212093841, 5.635514128831429, 3.8079037374246947, 5.801852205310435, 6.0580800621453035, 3.4017166894652107, 3.7612097181998236, 6.265622741143591, 4.645625775838694, 3.533008652244674, 5.342688143712924, 3.513283708157327, 3.9076948541870364, 3.3857129499362144, 3.326114100494354, 3.5946384555439908, 4.005439424802653, 3.3386216764160896], "Total": [136.0, 206.0, 104.0, 47.0, 38.0, 42.0, 54.0, 56.0, 62.0, 70.0, 87.0, 91.0, 34.0, 83.0, 13.0, 32.0, 33.0, 180.0, 40.0, 12.0, 16.0, 20.0, 17.0, 99.0, 20.0, 18.0, 14.0, 19.0, 20.0, 10.0, 61.03656029283421, 99.00825358047494, 20.292992823581123, 40.611195778048675, 21.12158413618391, 24.50520875788402, 21.09007915774174, 25.33071137974192, 180.89599953222742, 15.179142965027719, 16.005634108422047, 15.173830823190603, 13.474440965260152, 13.476430565151988, 14.3151420098233, 36.304892552838766, 12.619446576876168, 17.700698360728328, 21.000350761339188, 11.779795073613284, 13.467027179358515, 21.06229663758782, 12.61528092363537, 12.61825703299287, 20.13244406597652, 14.26474568579041, 15.949474319385473, 10.915325846857248, 10.077300414158563, 10.077076353542902, 85.84766289297924, 88.2009670799902, 51.41700976980525, 64.04979655152857, 27.59227236759724, 26.94708520947956, 21.887659686689535, 19.277080686365228, 65.39374016347564, 206.78663248086872, 76.41854196420816, 87.79871867337071, 94.24372005942666, 32.39863893654557, 105.03266900125614, 78.87459112434615, 37.634858879379024, 30.89877303101077, 86.48081643911456, 104.96034115055217, 52.21934201456519, 52.493826452545285, 87.78348211202922, 40.048591824682156, 31.8320200460868, 57.884921967408324, 59.360150836847474, 83.93847193446547, 91.78809124244931, 104.03168193878412, 61.40870630306816, 136.52405036789486, 54.095740494482236, 70.15985565573185, 44.54183019472885, 6.476346742857931, 5.03167166264567, 5.031871065795163, 5.032214499838286, 5.032552393098886, 5.040801813674889, 4.312764141326861, 4.314739326471342, 4.315723207107186, 4.316628670474077, 4.320284215206539, 3.5911132828363885, 3.594052482950045, 5.064824482610819, 3.59268449191926, 3.5971612171873932, 3.5993084018346795, 3.600883937031459, 3.6011035888398797, 3.6015925058497587, 5.076145962130191, 5.07683087096691, 5.077239431429748, 5.0784468105159, 4.347908176042906, 3.6104428341703336, 2.8732458251518267, 2.8733358611200432, 2.8735862807759713, 2.873974186751736, 12.393256608995701, 10.290883596562542, 7.83233357094815, 4.355873243795605, 47.93265130039247, 10.450868532625078, 5.854550528297354, 9.978528672501348, 5.877012844675261, 54.26160536350902, 5.101909486099749, 5.123359250131536, 9.803839312691245, 6.678044434259724, 20.23641387263781, 16.496336009097, 104.03168193878412, 13.795268348567761, 136.52405036789486, 32.41970443158634, 62.06370337080749, 18.43077357780033, 91.78809124244931, 83.93847193446547, 28.296009903413044, 12.303165690918453, 12.878790840381878, 56.00104188154259, 16.406708939721632, 70.15985565573185, 51.325000634601096, 61.40870630306816, 87.78348211202922, 26.142717145613734, 15.6903995332084, 104.96034115055217, 54.095740494482236, 59.360150836847474, 40.42363114161792, 105.03266900125614, 86.48081643911456, 40.62906900359434, 57.884921967408324, 34.99868110900359, 3.642421613972881, 2.443121019734237, 2.448963639325756, 2.4569708321119066, 3.720445632198129, 2.4730171039396125, 1.8643456754430416, 1.8651958243605196, 1.8653028228810613, 1.8660288147833697, 1.8669969137950466, 1.86704956658193, 1.8674468251113567, 1.8671909468070398, 1.8675453055170839, 1.8680541328769689, 1.868304488004068, 1.8680870853118439, 1.8685674881008714, 1.8690527780174642, 1.8692717726136914, 1.868764298628407, 4.936591270865137, 1.87073687796103, 1.8709005086507764, 1.8709038788777455, 1.871247607014797, 1.8713523033277664, 1.8712664810438338, 1.8715959445301156, 2.5172080540415127, 3.8754483526302583, 5.199281865971911, 38.30828459909589, 13.092776962034732, 6.491466361260124, 42.49894786981286, 4.721875989354699, 4.61361621717735, 3.2289771201348048, 136.52405036789486, 56.00104188154259, 7.025103823234707, 3.193813708259695, 3.3098802600576667, 62.06370337080749, 3.204094094397097, 5.482933597577416, 206.78663248086872, 3.3441668627019503, 70.15985565573185, 17.26560582271523, 8.81873168175647, 34.99868110900359, 16.761543019576656, 47.93265130039247, 54.26160536350902, 14.420043797407601, 19.867465474383934, 87.79871867337071, 40.76066289703135, 21.21619543387675, 91.78809124244931, 20.916525610635187, 33.74941199457646, 20.301430306757002, 22.50921928562174, 38.67000412815183, 94.24372005942666, 29.690338781343012], "Category": ["Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3"], "logprob": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.4631, -4.9848, -6.573, -5.8833, -6.5378, -6.3893, -6.5423, -6.3602, -4.3945, -6.873, -6.821, -6.8754, -6.9947, -6.9959, -6.9359, -6.0058, -7.0632, -6.7248, -6.5547, -7.1329, -7.0007, -6.5538, -7.0675, -7.0673, -6.6007, -6.9456, -6.8347, -7.2147, -7.2953, -7.2954, -5.1551, -5.1312, -5.6672, -5.452, -6.2883, -6.3166, -6.523, -6.6478, -5.4556, -4.339, -5.3072, -5.179, -5.1135, -6.145, -5.0226, -5.3015, -6.0105, -6.2002, -5.2342, -5.0556, -5.7088, -5.7118, -5.2438, -5.9784, -6.1824, -5.6688, -5.666, -5.3876, -5.3537, -5.2709, -5.6748, -5.1025, -5.7895, -5.6869, -5.9364, -6.4855, -6.755, -6.7552, -6.7556, -6.756, -6.7658, -6.9277, -6.9314, -6.9318, -6.9331, -6.9387, -7.1314, -7.1367, -6.7965, -7.1434, -7.1422, -7.1462, -7.1495, -7.1494, -7.1502, -6.81, -6.8115, -6.8114, -6.8129, -6.9798, -7.1674, -7.3969, -7.3974, -7.3981, -7.3984, -5.9448, -6.1462, -6.4142, -6.9919, -4.8769, -6.2574, -6.745, -6.3055, -6.749, -4.9714, -6.8734, -6.8755, -6.418, -6.6909, -5.917, -6.0682, -4.8404, -6.2181, -4.8427, -5.7491, -5.4844, -6.1576, -5.3089, -5.3695, -5.9617, -6.375, -6.3618, -5.6637, -6.2561, -5.6339, -5.7833, -5.7574, -5.7476, -6.1852, -6.3643, -5.7232, -5.9458, -5.9659, -6.066, -5.8664, -5.9164, -6.1015, -6.1168, -6.205, -6.5214, -6.956, -6.9624, -6.9713, -6.5776, -6.9905, -7.3084, -7.3099, -7.3099, -7.3112, -7.3125, -7.3128, -7.3133, -7.3134, -7.3143, -7.3142, -7.3146, -7.3151, -7.315, -7.3158, -7.3161, -7.3164, -6.3478, -7.3185, -7.3186, -7.3187, -7.3193, -7.3193, -7.3196, -7.3197, -7.0417, -6.6997, -6.4952, -5.0032, -5.8338, -6.3989, -5.0991, -6.6951, -6.7195, -6.9566, -4.6328, -5.1905, -6.504, -6.9913, -6.9808, -5.3073, -7.0028, -6.7317, -4.848, -7.0135, -5.5091, -6.2095, -6.5455, -5.8713, -6.2634, -5.8423, -5.799, -6.3762, -6.2757, -5.7654, -6.0645, -6.3383, -5.9247, -6.3439, -6.2375, -6.3809, -6.3986, -6.321, -6.2128, -6.3949], "loglift": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.3336, 0.3282, 0.3249, 0.3209, 0.3202, 0.3201, 0.3172, 0.316, 0.3158, 0.3153, 0.3143, 0.3133, 0.3127, 0.3114, 0.311, 0.3105, 0.3098, 0.3098, 0.309, 0.3089, 0.3073, 0.3069, 0.3059, 0.3058, 0.3052, 0.3049, 0.3041, 0.3033, 0.3026, 0.3025, 0.3006, 0.2974, 0.301, 0.2965, 0.3024, 0.2977, 0.2993, 0.3015, 0.2722, 0.2376, 0.2648, 0.2542, 0.2489, 0.2851, 0.2314, 0.2389, 0.2698, 0.2773, 0.2141, 0.1991, 0.244, 0.2358, 0.1896, 0.2397, 0.2653, 0.181, 0.1586, 0.0905, 0.035, -0.0074, 0.1159, -0.1107, 0.128, -0.0295, 0.1754, 1.5546, 1.5375, 1.5372, 1.5368, 1.5363, 1.5248, 1.5189, 1.5148, 1.5142, 1.5127, 1.5062, 1.4983, 1.4923, 1.4894, 1.4859, 1.4859, 1.4813, 1.4776, 1.4776, 1.4766, 1.4737, 1.4721, 1.4721, 1.4704, 1.4587, 1.457, 1.4558, 1.4554, 1.4546, 1.4541, 1.4462, 1.4307, 1.4358, 1.4448, 1.1615, 1.3042, 1.396, 1.3023, 1.3882, 0.943, 1.4052, 1.3989, 1.2075, 1.3185, 0.9838, 1.0369, 0.4231, 1.0658, 0.1491, 0.6804, 0.2957, 0.8366, 0.0798, 0.1087, 0.6038, 1.0234, 0.9908, 0.2192, 0.8544, 0.0236, 0.1868, 0.0333, -0.3143, 0.4595, 0.7909, -0.4686, -0.0283, -0.1413, 0.1428, -0.6125, -0.4681, 0.1023, -0.267, 0.1479, 2.0941, 2.0589, 2.0501, 2.038, 2.0168, 2.0123, 1.977, 1.975, 1.9749, 1.9732, 1.9714, 1.9711, 1.9704, 1.9704, 1.9694, 1.9691, 1.9686, 1.9682, 1.9681, 1.967, 1.9666, 1.9665, 1.9638, 1.9634, 1.9632, 1.9631, 1.9623, 1.9622, 1.962, 1.9617, 1.9434, 1.8539, 1.7645, 1.2593, 1.5024, 1.6389, 1.0597, 1.6609, 1.6598, 1.7795, 0.3589, 0.6923, 1.4548, 1.7557, 1.7305, 0.4727, 1.741, 1.4749, -0.2715, 1.6875, 0.1483, 0.85, 1.1858, 0.4816, 0.8258, 0.1962, 0.1154, 0.8635, 0.6435, -0.3322, 0.136, 0.5152, -0.536, 0.5238, 0.1518, 0.5167, 0.3957, -0.0678, -0.8504, 0.1225]}, "token.table": {"Topic": [3, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 3, 1, 2, 3, 1, 1, 2, 3, 1, 1, 1, 2, 2, 1, 2, 3, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 3, 1, 2, 3, 2, 1, 2, 3, 2, 3, 3, 1, 2, 1, 2, 3, 2, 1, 2, 1, 2, 3, 1, 2, 3, 3, 3, 1, 3, 3, 1, 2, 3, 1, 2, 1, 2, 1, 2, 1, 3, 1, 2, 3, 3, 2, 1, 2, 1, 2, 3, 3, 1, 2, 3, 1, 2, 3, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 3, 3, 2, 1, 1, 2, 3, 1, 1, 1, 2, 3, 3, 1, 1, 2, 1, 2, 3, 3, 1, 2, 1, 2, 1, 2, 1, 2, 3, 1, 2, 1, 3, 3, 1, 3, 3, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 3, 1, 2, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 2, 1, 1, 2, 3, 2, 1, 1, 2, 3, 1, 1, 2, 1, 2, 3, 1, 3, 1, 3, 2, 3, 1, 1, 2, 1, 2, 3, 1, 2, 3, 3, 1, 2, 1, 2, 1, 2, 3, 1, 1, 2, 3, 2, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 2, 2, 3, 1, 2, 3, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 3, 2, 3, 1, 2, 1, 2, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 2, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 2, 2, 2, 3, 2, 1, 2, 1, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 1, 2, 1, 3, 3], "Freq": [0.8166719864206053, 0.8330779512417388, 0.5354904817385467, 0.5258999608619013, 0.3346636114575735, 0.19123634940432774, 0.19691059832098817, 0.7876423932839527, 0.7630005008792478, 0.22151627444881386, 0.02461291938320154, 0.9259648239778397, 0.030865494132594658, 0.6841519206586131, 0.19954431019209548, 0.11402532010976885, 0.9509133326011903, 0.7108198554989579, 0.1332787229060546, 0.1332787229060546, 0.9881980843424125, 0.9604140838712368, 0.43493173517156886, 0.5799089802287585, 0.8347123516509033, 0.3637171306823082, 0.5455756960234623, 0.0606195217803847, 0.30600256739382153, 0.6120051347876431, 0.9523839775341834, 0.04683855627217295, 0.19697327435482295, 0.7878930974192918, 0.9936661600866901, 0.9338023226431142, 0.6371047800435719, 0.11583723273519488, 0.23167446547038975, 0.36476823299185707, 0.18238411649592853, 0.36476823299185707, 0.9266490831977672, 0.6007914210529546, 0.3534067182664439, 0.03534067182664439, 0.9268434994655707, 0.5343033590784259, 0.5351129624714883, 0.8953433799017921, 0.09524929573423321, 0.4698722531790112, 0.15662408439300374, 0.3393521828515081, 0.9274794236184137, 0.5735991604899346, 0.44613268038106024, 0.7743767369359359, 0.21444278868995145, 0.011913488260552859, 0.8949596434503231, 0.1047293199782293, 0.8087287373847606, 0.535316392817756, 0.5343974309004765, 0.9404698675095859, 0.5352446597547448, 0.5345010031193298, 0.9674066892166013, 0.027640191120474326, 0.005528038224094865, 0.8989080104887237, 0.09987866783208041, 0.2299956575693175, 0.91998263027727, 0.965320692299905, 0.9937055386819463, 0.3972655332937007, 0.7945310665874014, 0.39532696110831234, 0.4941587013853904, 0.09883174027707808, 0.8140104773978476, 0.8350301861317524, 0.19518443879849462, 0.7807377551939785, 0.6285951156696699, 0.2285800420616981, 0.17143503154627357, 0.5344028210116463, 0.48831374125502813, 0.43405665889335837, 0.054257082361669796, 0.38823520483944296, 0.5435292867752202, 0.07764704096788859, 0.9855618722123306, 0.2870574814557017, 0.6698007900633041, 0.19600504531186203, 0.7840201812474481, 0.9646471250047497, 0.17015446901839396, 0.8507723450919698, 0.5485560835549692, 0.42665473165386497, 0.8531306377369509, 0.15715564379364885, 0.8339909775702765, 0.96485389042573, 0.03710976501637423, 0.9523650451029231, 0.5345487180911987, 0.5361059811486512, 0.7935245518180539, 0.9161430579627826, 0.8669941332362111, 0.12385630474803017, 0.019054816115081565, 0.9923292537701907, 0.9779854080660154, 0.8592742298639249, 0.029630145857376722, 0.11852058342950689, 0.5361367353172415, 0.9510029767680023, 0.16137808350940552, 0.8068904175470275, 0.7046696529082114, 0.10066709327260162, 0.20133418654520324, 0.5351693242914953, 0.6169087704733702, 0.3701452622840221, 0.4063994686904636, 0.5689592561666491, 0.9495640643626657, 0.047478203218133286, 0.93854859449904, 0.03236374463789793, 0.03236374463789793, 0.9869442521852582, 0.03947777008741033, 0.9814405604122244, 0.5355638649116385, 0.5363812157648076, 0.2990281409558735, 0.598056281911747, 0.823627882201101, 0.993598345253502, 0.5535611295513647, 0.24999534882964858, 0.19642491693758105, 0.9523705099947452, 0.03401323249981233, 0.01133774416660411, 0.8946419687796582, 0.029015415203664588, 0.07737444054310558, 0.3121006969641341, 0.6242013939282682, 0.9422928149452775, 0.03624203134404914, 0.2295750918428066, 0.9183003673712264, 0.9529920199438622, 0.03889763346709642, 0.5882498568336931, 0.11764997136673862, 0.28235993128017267, 0.9551803419765093, 0.034945622267433266, 0.011648540755811088, 0.911032349125611, 0.06282981718107662, 0.8329648607185173, 0.9371699926657145, 0.1974402081322509, 0.7897608325290036, 0.5356195249232089, 0.830923002465815, 0.9885440384029501, 0.9299888731395584, 0.05314222132226048, 0.02657111066113024, 0.9830173868274831, 0.878807614559312, 0.11563258086306737, 0.9160080551233968, 0.03925748807671701, 0.03925748807671701, 0.8446098087572795, 0.1583643391419899, 0.9483131784575785, 0.5358974052692169, 0.6960550721071599, 0.8186250226022616, 0.9647895622175824, 0.9942436071366626, 0.9270548455754011, 0.9797163013400432, 0.010100168055051992, 0.010100168055051992, 0.19233425418705746, 0.19233425418705746, 0.5770027625611724, 0.5354622439658143, 0.17080730538862124, 0.8540365269431063, 0.8543748572685355, 0.13669997716296567, 0.9019168592496358, 0.0530539328970374, 0.04244314631762992, 0.9437502936918458, 0.8420247469762671, 0.03368098987905068, 0.10104296963715205, 0.9258650127509662, 0.4235604671763783, 0.4235604671763783, 0.3096955979540207, 0.3096955979540207, 0.6193911959080414, 0.2167497149582608, 0.2167497149582608, 0.4334994299165216, 0.9849500669374763, 0.02462375167343691, 0.12767586964237837, 0.7660552178542703, 0.12767586964237837, 0.9594447419506735, 0.04568784485479398, 0.7881218100517114, 0.049257613128231965, 0.1477728393846959, 0.7159245414330048, 0.0596603784527504, 0.2386415138110016, 0.9935316335417541, 0.6959944141506439, 0.5343729228439411, 0.7114702740055806, 0.17173420407031256, 0.12266728862165183, 0.5353069500146284, 0.20043035056977535, 0.7015062269942136, 0.10021517528488767, 0.5693866027674167, 0.4270399520755625, 0.6934791697233107, 0.06934791697233107, 0.20804375091699323, 0.19434677122070051, 0.7773870848828021, 0.5349676888351872, 0.2580346605112934, 0.5160693210225868, 0.695900474409086, 0.5350303703358857, 0.7948869838353724, 0.18485743810124938, 0.8465071444268178, 0.15548090407839513, 0.5800491760041038, 0.2577996337796017, 0.16112477111225104, 0.3131053002289538, 0.6262106004579076, 0.45826794555488615, 0.07637799092581436, 0.45826794555488615, 0.1969998513558016, 0.7879994054232063, 0.6960768836736471, 0.1540483989823649, 0.3080967979647298, 0.46214519694709466, 0.6502767063312808, 0.3060125676853086, 0.038251570960663575, 0.9793836174637166, 0.9000496403591337, 0.0765999693922667, 0.03829998469613335, 0.9923513178982907, 0.7208962404777098, 0.23380418610087886, 0.03896736435014648, 0.9111750286199102, 0.022779375715497752, 0.06833812714649326, 0.7299421863237794, 0.206998530450027, 0.05447329748684921, 0.9001631449102306, 0.07607012492199132, 0.02535670830733044, 0.30212573308696594, 0.6042514661739319, 0.2687850055771883, 0.8063550167315648, 0.9328109976200799, 0.06116793427016917, 0.25035126733959207, 0.6258781683489801, 0.12517563366979603, 0.1133949910358113, 0.5669749551790565, 0.3401849731074339, 0.8254696005520177, 0.16846318378612604, 0.1969574241091877, 0.7878296964367508, 0.8353955343983172, 0.833128768508206, 0.20256892765293694, 0.6077067829588108, 0.9264482335842745, 0.9640573911370319, 0.027544496889629483, 0.9337513440368327, 0.7017093123896214, 0.29798614635723647, 0.6127394537119685, 0.23566902065844944, 0.18853521652675956, 0.7240754334343593, 0.18101885835858983, 0.1034393476334799, 0.29948887278131814, 0.7487221819532953, 0.742140157941271, 0.22264204738238128, 0.024738005264709033, 0.6299256414401243, 0.22706621958888198, 0.13916961845770187, 0.3870139827105543, 0.4975894063421412, 0.11057542363158694, 0.8334934562625439, 0.797932458602402, 0.19541203067813928, 0.9512273307776595, 0.535604419881971, 0.5345019659656636], "Term": ["ab", "accuse", "alta", "amazon", "amazon", "amazon", "appreciate_good_luck_wonderful", "appreciate_good_luck_wonderful", "ask", "ask", "ask", "bad", "bad", "barista", "barista", "barista", "bean", "big", "big", "big", "bitch", "black", "book", "book", "booth", "break", "break", "break", "bro", "bro", "buy", "buy", "bye", "bye", "card_paypal", "case", "ceo", "ceo", "ceo", "child", "child", "child", "circle", "close", "close", "close", "clothe", "cody", "collar", "come", "come", "company", "company", "company", "contract", "couple", "couple", "cup", "cup", "cup", "day", "day", "debate", "decatur", "dick", "different", "district", "diversity", "drink", "drink", "drink", "drive", "drive", "dutch", "dutch", "early", "earn_complete_survey_point", "earning", "earning", "eat", "eat", "eat", "editing", "election", "elon", "elon", "employee", "employee", "employee", "encourage", "end", "end", "end", "enter", "enter", "enter", "espresso", "fair", "fair", "favourite", "favourite", "feel_like", "foam", "foam", "food", "food", "free", "free", "freediscounte", "fuck", "fuck", "fucking", "fundamental", "galaxy", "gc", "going", "good", "good", "good", "good_luck_wonderful_wednesday", "good_morning", "guy", "guy", "guy", "hamilton", "hand", "hard", "hard", "help", "help", "help", "hilarious", "hour", "hour", "huge", "huge", "ice", "ice", "iced", "iced", "iced", "ill", "ill", "imagine", "impossible", "indians", "instagram", "instagram", "invest", "invite_online_pay_survey", "job", "job", "job", "know", "know", "know", "like", "like", "like", "link_bio", "link_bio", "live", "live", "loaf", "loaf", "lol", "lol", "look", "look", "look", "love", "love", "love", "man", "man", "mask", "maybe", "meeting", "meeting", "midtown", "minimum", "mocha", "money", "money", "money", "morning", "need", "need", "new", "new", "new", "nft", "nft", "nice", "nolan", "nut", "nv", "odyssey", "offer", "office", "order", "order", "order", "organize", "organize", "organize", "overtime", "pass", "pass", "pay", "pay", "people", "people", "people", "person", "place", "place", "place", "pod", "policy", "policy", "progressive", "progressive", "progressive", "public", "public", "public", "pumpkin_spice", "pumpkin_spice", "raise", "raise", "raise", "read", "read", "real", "real", "real", "reason", "reason", "reason", "redeem_amazon_itunes_gift", "report", "rhode", "right", "right", "right", "schultz", "seattle", "seattle", "seattle", "september", "september", "serve", "serve", "serve", "set", "set", "shareholder", "similar", "similar", "simply", "sob", "start", "start", "stop", "stop", "store", "store", "store", "supervisor", "supervisor", "support", "support", "support", "support_entry", "support_entry", "svg", "table", "table", "table", "talk", "talk", "talk", "taste", "tell", "tell", "tell", "thank_support_entry_appreciate", "thing", "thing", "thing", "think", "think", "think", "time", "time", "time", "today", "today", "today", "training", "training", "trust", "trust", "try", "try", "union", "union", "union", "unionized", "unionized", "unionized", "use", "use", "usual_good_morning_thank", "usual_good_morning_thank", "verismo", "victory", "vote", "vote", "wage", "walk", "walk", "wan_na", "want", "want", "watch", "watch", "watch", "way", "way", "way", "wednesday", "wednesday", "week", "week", "week", "work", "work", "work", "worker", "worker", "worker", "xmas", "year", "year", "yesterday", "yoga", "zayn"]}, "R": 30, "lambda.step": 0.01, "plot.opts": {"xlab": "PC1", "ylab": "PC2"}, "topic.order": [1, 2, 3]};

function LDAvis_load_lib(url, callback){
  var s = document.createElement('script');
  s.src = url;
  s.async = true;
  s.onreadystatechange = s.onload = callback;
  s.onerror = function(){console.warn("failed to load library " + url);};
  document.getElementsByTagName("head")[0].appendChild(s);
}

if(typeof(LDAvis) !== "undefined"){
   // already loaded: just create the visualization
   !function(LDAvis){
       new LDAvis("#" + "ldavis_el555510858257424942357348", ldavis_el555510858257424942357348_data);
   }(LDAvis);
}else if(typeof define === "function" && define.amd){
   // require.js is available: use it to load d3/LDAvis
   require.config({paths: {d3: "https://d3js.org/d3.v5"}});
   require(["d3"], function(d3){
      window.d3 = d3;
      LDAvis_load_lib("https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js", function(){
        new LDAvis("#" + "ldavis_el555510858257424942357348", ldavis_el555510858257424942357348_data);
      });
    });
}else{
    // require.js not available: dynamically load d3 & LDAvis
    LDAvis_load_lib("https://d3js.org/d3.v5.js", function(){
         LDAvis_load_lib("https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js", function(){
                 new LDAvis("#" + "ldavis_el555510858257424942357348", ldavis_el555510858257424942357348_data);
            })
         });
}
</script></div></div>
</div>
<p>There is high intertopic distances between topics, though topic 3 and topic 4 have some overlap between them. But the overall result of the model is satisfactory.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Get the dominant topic number for each 
def dominant_topics(ldamodel, corpus, tweets, texts):
    sent_topics_df = pd.DataFrame()
    for row in tqdm(ldamodel[corpus]):
        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)
        for j, (topic_num, prop_topic) in enumerate(row):
            if j==0:
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = &quot;, &quot;.join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(
                    pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True
                )
            else:
                break
    sent_topics_df.columns = [&#39;Dominant_Topic&#39;, &#39;Perc_Contribution&#39;, &#39;Topic_Keywords&#39;]
    
    # Add tweets column
    contents = pd.Series(tweets)
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    sent_topics_df.rename(columns={0: &quot;Tweet&quot;}, inplace=True)

    # Add trigrams column
    contents = pd.Series(texts)
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    sent_topics_df.rename(columns={0: &quot;Texts&quot;}, inplace=True)
    return sent_topics_df.copy()

topics_df = dominant_topics(ldamodel=lda_model, 
                            corpus=bow_corpus, 
                            tweets=df[&#39;tweet&#39;].values.tolist(), 
                            texts=df[&#39;trigram&#39;].values.tolist())
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████| 2770/2770 [00:06&lt;00:00, 458.83it/s]
</pre></div>
</div>
</div>
</div>
<p>The dominant topic for each of the tweet is determined and the sample result is as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>topics_df.head()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Dominant_Topic</th>
      <th>Perc_Contribution</th>
      <th>Topic_Keywords</th>
      <th>Tweet</th>
      <th>Texts</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0.9547</td>
      <td>like, drink, order, day, good, work, people, k...</td>
      <td>So now you up 3am for work. These boomers 🤬 th...</td>
      <td>[work, boomer, wake, early, bullshxt, stop, bu...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0.8630</td>
      <td>like, drink, order, day, good, work, people, k...</td>
      <td>@ScottPalmer61 Yes. It’s a special Starbucks a...</td>
      <td>[yes, special, attachment]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0.8656</td>
      <td>like, drink, order, day, good, work, people, k...</td>
      <td>I like the caramel frappe from Starbucks https...</td>
      <td>[like, caramel, frappe]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0.9018</td>
      <td>like, drink, order, day, good, work, people, k...</td>
      <td>Why is No Time To Die playing on repeat at #St...</td>
      <td>[time, die, playing, repeat, notice]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0.9311</td>
      <td>like, drink, order, day, good, work, people, k...</td>
      <td>https://t.co/tgR9Z5p8Ts | Criminals steal 200 ...</td>
      <td>[criminal, steal, customer, datum, singapore, ...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]

cloud = WordCloud(background_color=&#39;white&#39;,
                  width=2500,
                  height=1800,
                  max_words=20,
                  colormap=&#39;tab10&#39;,
                  color_func=lambda *args, **kwargs: cols[i],
                  prefer_horizontal=1.0)

topics = lda_model.show_topics(formatted=False)

fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)

for i, ax in enumerate(axes.flatten()):
    try:
        fig.add_subplot(ax)
        topic_words = dict(topics[i][1])
        cloud.generate_from_frequencies(topic_words, max_font_size=300)
        plt.gca().imshow(cloud)
        plt.gca().set_title(&#39;Topic &#39; + str(i), fontdict=dict(size=16))
        plt.gca().axis(&#39;off&#39;)
    except:
        pass


plt.subplots_adjust(wspace=0, hspace=0)
plt.axis(&#39;off&#39;)
plt.margins(x=0, y=0)
plt.tight_layout()
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.
</pre></div>
</div>
<img alt="../../_images/lda_23_1.png" src="../../_images/lda_23_1.png" />
</div>
</div>
<p>The above graph shows the word cloud of the dominant words in each topic. The topics can be attributed as follows:</p>
<ol class="simple">
<li><p>The first topic is related to general like of coffee</p></li>
<li><p>The second topic is related to the company workers</p></li>
<li><p>The third topic is related to the brand appreciation</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from collections import Counter
topics = lda_model.show_topics(formatted=False)
data_flat = [w for w_list in df[&#39;trigram&#39;].values.tolist() for w in w_list]
counter = Counter(data_flat)

out = []
for i, topic in topics:
    for word, weight in topic:
        out.append([word, i , weight, counter[word]])

df = pd.DataFrame(out, columns=[&#39;word&#39;, &#39;topic_id&#39;, &#39;importance&#39;, &#39;word_count&#39;])        

# Plot Word Count and Weights of Topic Keywords
fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)
cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]
for i, ax in enumerate(axes.flatten()):
    ax.bar(x=&#39;word&#39;, height=&quot;word_count&quot;, data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label=&#39;Word Count&#39;)
    ax_twin = ax.twinx()
    ax_twin.bar(x=&#39;word&#39;, height=&quot;importance&quot;, data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label=&#39;Weights&#39;)
    ax.set_ylabel(&#39;Word Count&#39;, color=cols[i])
    ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)
    ax.set_title(&#39;Topic: &#39; + str(i), color=cols[i], fontsize=16)
    ax.tick_params(axis=&#39;y&#39;, left=False)
    ax.set_xticklabels(df.loc[df.topic_id==i, &#39;word&#39;], rotation=30, horizontalalignment= &#39;right&#39;)
    ax.legend(loc=&#39;upper left&#39;); ax_twin.legend(loc=&#39;upper right&#39;)

fig.tight_layout(w_pad=2)    
fig.suptitle(&#39;Word Count and Importance of Topic Keywords&#39;, fontsize=22, y=1.05)    
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/var/folders/zl/bqtzmz3s0xl5_ddgbjsqxsww0000gn/T/ipykernel_5555/3296003400.py:24: UserWarning:

FixedFormatter should only be used together with FixedLocator

/var/folders/zl/bqtzmz3s0xl5_ddgbjsqxsww0000gn/T/ipykernel_5555/3296003400.py:24: UserWarning:

FixedFormatter should only be used together with FixedLocator

/var/folders/zl/bqtzmz3s0xl5_ddgbjsqxsww0000gn/T/ipykernel_5555/3296003400.py:24: UserWarning:

FixedFormatter should only be used together with FixedLocator
</pre></div>
</div>
<img alt="../../_images/lda_25_1.png" src="../../_images/lda_25_1.png" />
</div>
</div>
<p>The above graph is related to the frequency of the words and their corresponding weights in the topics. It can be observed that all words in all the topics have relatively low weightages eventhough certain words have high frequency. But one of the most intriguing result is that no topic is dominated by any single word and the dominant words contribute equally in the model.</p>
</section>
<section id="lda-with-tf-idf">
<h2>LDA with TF-IDF<a class="headerlink" href="#lda-with-tf-idf" title="Permalink to this headline">#</a></h2>
<p>The other way to convert text into numbers is term frequency - inverse document frequency (tf-idf) model.</p>
<p>TF-IDF is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining.</p>
<p>The term frequency (TF) of a word is the number of times the word appears in a document, normalized by the total number of words in the document. The inverse document frequency (IDF) of a word is the logarithmically scaled inverse fraction of the documents that contain the word. The product of these two quantities is the TD-IDF weight of a word in a document.</p>
<p>TD-IDF is typically used to weight the importance of words in a document, in relation to a corpus of documents as a whole. It is often used as a weighting factor in information retrieval and text mining, as it can help to identify the most relevant words in a document. For example, if a certain word appears frequently in a document but not in many other documents, it may be more important to the meaning of the document than a word that appears frequently in many documents.</p>
<p>Before creating the LDA model, it is necessary to create the corpus using tf-idf.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>df = main_df.copy()
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Create tf-idf corpus
tfidf = gensim.models.TfidfModel(bow_corpus)
tfidf_corpus = tfidf[bow_corpus]
</pre></div>
</div>
</div>
</div>
<p>The rest of the procedure remains the same for creating the LDA model remains the same.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>## Find the appropriate number of topics

# Declare the range for the number of topics to be searched for
topics_range = range(3, 20, 1)

# Initialize a blank variable object for storing the results
model_results = {
    &#39;Number of topics&#39;: [],
    &#39;Coherence Score&#39;: []
}

# Interate over the range
for k in tqdm(topics_range):

    # Build the LDA model
    lda_model = gensim.models.LdaMulticore(corpus=tfidf_corpus,
                                            id2word=id2word,
                                            num_topics=k,
                                            random_state=14,
                                            chunksize=400,
                                            passes=10,
                                            alpha=&#39;asymmetric&#39;,
                                            eta=&#39;auto&#39;,
                                            per_word_topics=True)
    
    # Build the coherence model from the LDA model
    coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, 
                                                    corpus=tfidf_corpus, 
                                                    dictionary=id2word,
                                                    coherence=&#39;u_mass&#39;)
    # Get the coherence score
    coherence_score = coherence_model_lda.get_coherence()

    # Append the values to the results object
    model_results[&#39;Number of topics&#39;].append(k)
    model_results[&#39;Coherence Score&#39;].append(coherence_score)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████| 17/17 [02:12&lt;00:00,  7.80s/it]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Convert the result dictionary to dataframe
model_results = pd.DataFrame(model_results)

# Get the best k and the corresponding coherence score
best_coherence = model_results[&#39;Coherence Score&#39;].max()
best_k_index = model_results[model_results[&#39;Coherence Score&#39;] == best_coherence].index
best_k = model_results.iloc[best_k_index][&quot;Number of topics&quot;].values[0]

model_results
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Number of topics</th>
      <th>Coherence Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>-8.882971</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4</td>
      <td>-10.162568</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5</td>
      <td>-10.622884</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6</td>
      <td>-12.507163</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7</td>
      <td>-10.490004</td>
    </tr>
    <tr>
      <th>5</th>
      <td>8</td>
      <td>-12.156693</td>
    </tr>
    <tr>
      <th>6</th>
      <td>9</td>
      <td>-12.924883</td>
    </tr>
    <tr>
      <th>7</th>
      <td>10</td>
      <td>-13.261529</td>
    </tr>
    <tr>
      <th>8</th>
      <td>11</td>
      <td>-13.747339</td>
    </tr>
    <tr>
      <th>9</th>
      <td>12</td>
      <td>-15.384174</td>
    </tr>
    <tr>
      <th>10</th>
      <td>13</td>
      <td>-13.018680</td>
    </tr>
    <tr>
      <th>11</th>
      <td>14</td>
      <td>-13.889211</td>
    </tr>
    <tr>
      <th>12</th>
      <td>15</td>
      <td>-15.362030</td>
    </tr>
    <tr>
      <th>13</th>
      <td>16</td>
      <td>-13.154168</td>
    </tr>
    <tr>
      <th>14</th>
      <td>17</td>
      <td>-12.544566</td>
    </tr>
    <tr>
      <th>15</th>
      <td>18</td>
      <td>-12.540822</td>
    </tr>
    <tr>
      <th>16</th>
      <td>19</td>
      <td>-14.186214</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Plot the reuslts
fig = px.line(model_results, x=&quot;Number of topics&quot;, y=&quot;Coherence Score&quot;, title=&#39;Scree Plot for LDA using TF-IDF&#39;, markers=True)
fig.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/plotly/io/_renderers.py:395: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/plotly/io/_renderers.py:395: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.
</pre></div>
</div>
<img alt="../../_images/lda_35_1.svg" src="../../_images/lda_35_1.svg" /></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(f&quot;From the above graph, it can be observed that the best coherence score of {best_coherence} is obtained using {best_k} topics is closest to 0. Hence, for this data set the optimum number of topics is {best_k}.&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>From the above graph, it can be observed that the best coherence score of -8.882970855071166 is obtained using 3 topics is closest to 0. Hence, for this data set the optimum number of topics is 3.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Create final models with optimum k i.e. 5 for further analysis
lda_model = gensim.models.LdaMulticore(corpus=tfidf_corpus,
                                            id2word=id2word,
                                            num_topics=best_k,
                                            random_state=14,
                                            chunksize=400,
                                            passes=10,
                                            alpha=&#39;asymmetric&#39;,
                                            eta=&#39;auto&#39;,
                                            per_word_topics=True)
    
coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, 
                                                corpus=tfidf_corpus, 
                                                dictionary=id2word, 
                                                coherence=&#39;u_mass&#39;)

coherence_score = coherence_model_lda.get_coherence()
print(f&quot;The coherence for {best_k} number of topics is: {coherence_score}&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The coherence for 3 number of topics is: -8.882970855071166
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Plot the distribution of words in each topic
LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, tfidf_corpus, id2word)
pyLDAvis.display(LDAvis_prepared)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css">


<div id="ldavis_el5555111629572964624427546"></div>
<script type="text/javascript">

var ldavis_el5555111629572964624427546_data = {"mdsDat": {"x": [0.06162802760104901, -0.030840596142340145, -0.030787431458708867], "y": [-4.13083829259129e-07, -0.0007180580997821012, 0.0007184711836111218], "topics": [1, 2, 3], "cluster": [1, 1, 1], "Freq": [84.57916409699219, 9.086745005549554, 6.334090897458247]}, "tinfo": {"Term": ["deliver", "tim", "domino", "tier", "gay", "season", "sir", "invest", "tail", "rainy", "shadow", "karen", "manage", "drizzle", "eh", "tbh", "sweater", "crypto", "bye", "manger", "mask", "wear", "smile", "unit", "build", "grab", "applaud", "google", "trust", "overrate", "drink", "like", "work", "want", "good", "order", "love", "need", "day", "know", "morning", "people", "time", "cup", "today", "pay", "think", "new", "barista", "try", "buy", "stop", "store", "lol", "tell", "job", "come", "year", "thank", "worker", "manger", "domino", "overrate", "idkumstarbucks", "applaud", "humidity", "delhi", "amazoneth", "craze", "domain", "cokedao", "lla", "becos", "wlahy", "dool", "hayftah", "elly", "yesra", "elcaramel", "background", "literal", "targaryen", "crumb", "boi", "avellino", "pirate", "rainy", "jd", "hihi", "dpt", "karen", "gay", "coat", "tier", "tim", "deliver", "sweater", "tail", "shadow", "invest", "season", "jd", "dpt", "atleast", "hihi", "nk", "dh", "yg", "diversify", "jugak", "mckenzie", "ot", "mostyn", "restaurantsdid", "masquerading", "bike", "centrei", "hmv", "waterstones", "background", "literal", "targaryen", "crumb", "boi", "avellino", "pirate", "dool", "wlahy", "hayftah", "elly", "elcaramel", "fomc", "gox", "vasil", "cpi", "mt", "deadline", "merge", "hardfork", "tba", "hike", "hei", "falls", "bing", "want", "babble", "lattesily", "diploma", "discipline", "brock", "comprehensive", "lait", "au", "sunoco", "cafs", "fy", "rba", "devyanis", "pair", "capture", "gigi", "like", "love", "drink", "need", "know", "morning", "work", "good", "thank", "think", "stop", "buy", "day", "tell", "barista", "time", "new", "cup", "today", "pay", "lol", "try", "order", "union", "dunkin", "man", "people", "money", "good_morning"], "Freq": [1.0, 1.0, 0.0, 0.0, 0.0, 3.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.0, 1.0, 3.0, 1.0, 1.0, 2.0, 2.0, 0.0, 0.0, 1.0, 0.0, 27.257573366035544, 28.050595031215806, 24.888448821385392, 23.5602644820814, 18.896921591187134, 18.04390440393679, 18.76652566104761, 17.051126889147493, 17.070400946159637, 16.800721953289024, 15.977849427128827, 15.17759566054555, 15.515214325191307, 14.985356731660353, 14.700732422266112, 14.886335830758128, 15.809038811836036, 13.524730125204632, 13.478611205720144, 12.842873954128812, 12.891513909493542, 12.618730129431995, 11.264015255284493, 12.28805489795869, 11.194711786491183, 11.11758861607443, 10.595668757398972, 10.945978560430143, 10.786500268288941, 10.259706406911683, 0.30673274454776345, 0.4224659987750929, 0.29133760988860447, 0.2905990522453763, 0.2915164171302744, 0.24792695729378422, 0.247761933612852, 0.23720429465964937, 0.23716347809041483, 0.23713427417110183, 0.23668015753314717, 0.2304825517419467, 0.22983989322255116, 0.2148135549603804, 0.21480797261828163, 0.21474194936234803, 0.2146953781565055, 0.21468705633171012, 0.214680664205418, 0.20175799021248692, 0.20174141548069985, 0.20168054383059195, 0.20160159848646672, 0.20152554769009642, 0.20145187455795338, 0.20122460087083954, 0.3038435723815328, 0.17852433408575447, 0.17840693260717105, 0.1784012124294649, 0.29036469725393343, 0.3058257173334187, 0.23853411537017236, 0.31247225369784626, 0.4018918634632499, 0.41541150347173106, 0.2411872785176716, 0.2606944800811615, 0.23674600849457156, 0.22168403972789386, 0.23576455005224317, 0.07495577252466602, 0.07495832467540756, 0.07495796437177346, 0.07495639705096513, 0.0749574239163223, 0.07495707562280934, 0.07495753801247311, 0.07495878706507131, 0.07495812050334823, 0.07495802442237914, 0.07495907530797859, 0.07495939357618871, 0.07495947764703667, 0.07495855887276971, 0.07496171753462864, 0.07495895520676722, 0.07496248017732082, 0.07496130318544943, 0.07497465243509277, 0.07497429813651924, 0.07497527696139188, 0.07497270079040808, 0.07497444225797288, 0.07497432215676152, 0.07497587146238814, 0.07497658606459576, 0.0749726947853475, 0.07497271880558977, 0.07497614169011371, 0.07497708448462294, 0.07504780007787526, 0.0750467491922758, 0.07504620273176409, 0.07504618471658238, 0.07504585443825113, 0.07504577036740318, 0.07504570431173692, 0.07504566227631294, 0.0750454821244959, 0.07504541606882964, 0.07505727005839147, 0.07505597296530872, 0.0751032988476475, 0.07798105598331044, 0.07510030232242393, 0.07513582225568544, 0.07505568472240144, 0.07505557062625064, 0.07505558864143234, 0.07505468788234711, 0.07507407822292216, 0.07507360982819782, 0.07508217904962879, 0.07504801025499515, 0.07504788414872321, 0.07504764394630048, 0.0750471695465156, 0.07506913005301387, 0.07504665311130672, 0.07506477638410185, 0.07782411372535787, 0.07742137232822355, 0.07724713549583426, 0.07675147779652658, 0.07657949286185041, 0.07656463634200443, 0.07655175548708541, 0.07654196723835908, 0.07648553167913819, 0.07643948487470044, 0.07635290391142666, 0.0763205006046001, 0.07626169905151528, 0.07623546294189237, 0.07621016362171812, 0.07618280456576892, 0.0761752321843923, 0.07617501600221184, 0.07617040411569538, 0.07616304791649922, 0.07611900680229129, 0.07610710477224492, 0.07610288321466539, 0.07608993029901957, 0.07603927761312594, 0.07602802412962094, 0.0760200554142468, 0.07599416159307629, 0.07598455950122757], "Total": [1.0, 1.0, 0.0, 0.0, 0.0, 3.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.0, 1.0, 3.0, 1.0, 1.0, 2.0, 2.0, 0.0, 0.0, 1.0, 0.0, 27.463459699954793, 28.263013272339542, 25.101910420007318, 23.78029409939598, 19.093006640529076, 18.236174930295434, 18.967667878280114, 17.243631727940578, 17.263684989727025, 16.996585517918746, 16.173426461256565, 15.367587767099142, 15.715088865504026, 15.17984595640858, 14.892402802458376, 15.083743980702279, 16.026154730414742, 13.714211800380344, 13.669609381432702, 13.031327537190073, 13.081116929996856, 12.813815952205747, 11.450956902129171, 12.494736073587088, 11.389247907215939, 11.311095592606764, 10.781708841808992, 11.139136817583816, 10.979099015276594, 10.451049623423588, 0.6163973499673276, 0.8557263222462992, 0.5952314122465971, 0.5960319176397393, 0.6100623068877772, 0.5487310829026991, 0.5489083052081708, 0.5422573164240955, 0.5423078352629916, 0.5423360150945317, 0.5428492720989618, 0.5461991653683057, 0.5468979142073229, 0.5142396310270129, 0.5142392429804808, 0.5143215247251381, 0.5143648766409636, 0.5143797314729395, 0.5143791504048407, 0.5049615036885413, 0.5049815803216263, 0.5050464964132992, 0.5051388373678891, 0.5052195468881764, 0.5053023762163067, 0.5055506089858733, 0.7664393378448348, 0.4776943743740566, 0.4778243085268783, 0.4778243247905456, 0.7791999364263171, 0.8609623118825409, 0.6743680287911012, 0.9576977295837054, 1.477192524386603, 1.7239445850992539, 0.7582842689691344, 0.9130797994462005, 0.9604639049199901, 1.2441252474202613, 3.6154736677337027, 0.4776943743740566, 0.4778243247905456, 0.47782666976839183, 0.4778243085268783, 0.4778694813785612, 0.47788332565955166, 0.4779300184713758, 0.4779503490496487, 0.4780103592130783, 0.48019982282321294, 0.4802748634483864, 0.4803491012975, 0.48036513959980376, 0.4804485516230438, 0.48047871529911756, 0.4804910881945998, 0.48060199485665606, 0.4806832991569014, 0.5049615036885413, 0.5049815803216263, 0.5050464964132992, 0.5051388373678891, 0.5052195468881764, 0.5053023762163067, 0.5055506089858733, 0.5142392429804808, 0.5142396310270129, 0.5143215247251381, 0.5143648766409636, 0.5143791504048407, 0.5161448516530378, 0.5161522666009009, 0.5161403016325803, 0.5161468898172061, 0.5161496734112716, 0.5161547644188014, 0.5161509475939605, 0.5161600777607193, 0.5161483016955407, 0.5161566245262205, 0.5226033160080257, 0.5226260760843886, 0.5476128151711518, 23.78029409939598, 0.5523910990710396, 0.5800829333731239, 0.5226271569899236, 0.5226155392531645, 0.5226295294962592, 0.5226396862220506, 0.536331977348284, 0.5363266393232976, 0.547303247215497, 0.5260592756681091, 0.5260483086621832, 0.5260507152719355, 0.5260708185818691, 0.5418491862894084, 0.5260587315546105, 0.5392269539104592, 28.263013272339542, 18.967667878280114, 27.463459699954793, 17.243631727940578, 16.996585517918746, 16.173426461256565, 25.101910420007318, 19.093006640529076, 10.979099015276594, 16.026154730414742, 12.813815952205747, 13.081116929996856, 17.263684989727025, 11.389247907215939, 13.669609381432702, 15.715088865504026, 13.714211800380344, 15.17984595640858, 14.892402802458376, 15.083743980702279, 12.494736073587088, 13.031327537190073, 18.236174930295434, 10.135151609149464, 8.080249547457788, 8.158094559200896, 15.367587767099142, 9.071547597363628, 6.363461257716767], "Category": ["Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3"], "logprob": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.309, -5.2804, -5.4, -5.4548, -5.6754, -5.7216, -5.6823, -5.7782, -5.777, -5.793, -5.8432, -5.8946, -5.8726, -5.9073, -5.9265, -5.9139, -5.8538, -6.0099, -6.0133, -6.0616, -6.0578, -6.0792, -6.1928, -6.1057, -6.1989, -6.2058, -6.2539, -6.2214, -6.2361, -6.2862, -7.5653, -7.2452, -7.6168, -7.6193, -7.6162, -7.7781, -7.7788, -7.8223, -7.8225, -7.8226, -7.8246, -7.8511, -7.8539, -7.9215, -7.9215, -7.9218, -7.922, -7.9221, -7.9221, -7.9842, -7.9843, -7.9846, -7.985, -7.9853, -7.9857, -7.9868, -7.5747, -8.1065, -8.1072, -8.1072, -7.6201, -7.5682, -7.8167, -7.5467, -7.2951, -7.262, -7.8057, -7.7279, -7.8243, -7.89, -7.8284, -8.6135, -8.6135, -8.6135, -8.6135, -8.6135, -8.6135, -8.6135, -8.6135, -8.6135, -8.6135, -8.6134, -8.6134, -8.6134, -8.6135, -8.6134, -8.6134, -8.6134, -8.6134, -8.6132, -8.6132, -8.6132, -8.6133, -8.6132, -8.6132, -8.6132, -8.6132, -8.6133, -8.6133, -8.6132, -8.6132, -8.6123, -8.6123, -8.6123, -8.6123, -8.6123, -8.6123, -8.6123, -8.6123, -8.6123, -8.6123, -8.6121, -8.6122, -8.6115, -8.5739, -8.6116, -8.6111, -8.6122, -8.6122, -8.6122, -8.6122, -8.6119, -8.6119, -8.6118, -8.6123, -8.6123, -8.6123, -8.6123, -8.612, -8.6123, -8.612, -8.5759, -8.5811, -8.5834, -8.5898, -8.5921, -8.5923, -8.5924, -8.5926, -8.5933, -8.5939, -8.595, -8.5954, -8.5962, -8.5966, -8.5969, -8.5973, -8.5974, -8.5974, -8.5974, -8.5975, -8.5981, -8.5982, -8.5983, -8.5985, -8.5991, -8.5993, -8.5994, -8.5997, -8.5999], "loglift": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.16, 0.1599, 0.1589, 0.1582, 0.1572, 0.1569, 0.1568, 0.1563, 0.1562, 0.1559, 0.1553, 0.155, 0.1547, 0.1546, 0.1545, 0.1543, 0.1538, 0.1536, 0.1534, 0.1529, 0.1529, 0.1521, 0.151, 0.1508, 0.1503, 0.1502, 0.1501, 0.15, 0.1498, 0.149, 1.7004, 1.6925, 1.6839, 1.68, 1.6599, 1.6039, 1.6029, 1.5715, 1.5713, 1.5711, 1.5682, 1.5355, 1.5315, 1.5254, 1.5254, 1.5249, 1.5246, 1.5246, 1.5245, 1.4809, 1.4808, 1.4804, 1.4798, 1.4793, 1.4787, 1.4771, 1.4731, 1.4141, 1.4132, 1.4131, 1.4112, 1.3633, 1.3591, 1.2783, 1.0966, 0.9753, 1.2529, 1.1449, 0.9979, 0.6734, -0.3318, 0.9072, 0.9069, 0.9069, 0.9069, 0.9068, 0.9068, 0.9067, 0.9067, 0.9065, 0.9019, 0.9018, 0.9017, 0.9016, 0.9014, 0.9014, 0.9014, 0.9012, 0.901, 0.8519, 0.8518, 0.8517, 0.8515, 0.8514, 0.8512, 0.8507, 0.8337, 0.8337, 0.8335, 0.8335, 0.8334, 0.831, 0.8309, 0.8309, 0.8309, 0.8309, 0.8309, 0.8309, 0.8309, 0.8309, 0.8309, 0.8187, 0.8186, 0.7725, -2.9609, 0.7638, 0.7154, 0.8186, 0.8186, 0.8186, 0.8185, 0.7929, 0.7929, 0.7728, 0.8119, 0.812, 0.8119, 0.8119, 0.7826, 0.8119, 0.7874, -3.1356, -2.742, -3.1144, -2.6554, -2.6432, -2.5938, -3.0335, -2.76, -2.2074, -2.5863, -2.3637, -2.3848, -2.663, -2.2474, -2.4302, -2.57, -2.4339, -2.5355, -2.5164, -2.5293, -2.3415, -2.3837, -2.7199, -2.1326, -1.9067, -1.9164, -2.5498, -2.023, -1.6686]}, "token.table": {"Topic": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "Freq": [0.9510147391378845, 0.7852550674317204, 0.9937989293704085, 1.0178499781069774, 1.0202464341593538, 1.1473840270242435, 0.9881523200614133, 0.9847260309786737, 0.5800650488672328, 0.9831244968762782, 0.9308858674960093, 0.9900684320469983, 1.00317562445113, 0.9951287588026262, 0.9428830878358835, 1.0701215442279106, 1.0571544224498444, 0.8037775956026422, 0.9724964226444958, 1.0002008922367174, 0.9906940824106343, 0.9604044398638462, 1.0017045913038634, 0.9806211416092754, 0.7885264137736746, 0.8085137976809094, 0.99211296676827, 0.9892770736199897, 0.9858712055682672, 1.0208388352009943, 0.987049097127102, 0.9944480640344057, 0.9760803209540719, 0.829766795641053, 1.0411635407405584, 0.5856500937283335, 0.6672980764519306, 1.0145299455282253, 0.9606184089256924, 1.09519452802101, 0.843486044392253, 0.9658232123501921, 1.0019037067335237, 0.9983679971362622, 1.0441707953455028, 0.6769598298740681, 1.0181297819525144, 1.0072249722874715, 1.0565932867165828, 0.9975959826732412, 0.9866650629056741, 0.8841957170146131, 1.009238990051414, 0.793974277876611, 0.9959401329101194, 0.9568416915356841, 0.9875091921517491], "Term": ["barista", "build", "buy", "bye", "come", "crypto", "cup", "day", "deliver", "drink", "drizzle", "dunkin", "eh", "good", "good_morning", "google", "grab", "invest", "job", "know", "like", "lol", "love", "man", "manage", "mask", "money", "morning", "need", "new", "order", "pay", "people", "season", "shadow", "sir", "smile", "stop", "store", "tail", "tbh", "tell", "thank", "think", "tier", "tim", "time", "today", "trust", "try", "union", "unit", "want", "wear", "work", "worker", "year"]}, "R": 30, "lambda.step": 0.01, "plot.opts": {"xlab": "PC1", "ylab": "PC2"}, "topic.order": [1, 2, 3]};

function LDAvis_load_lib(url, callback){
  var s = document.createElement('script');
  s.src = url;
  s.async = true;
  s.onreadystatechange = s.onload = callback;
  s.onerror = function(){console.warn("failed to load library " + url);};
  document.getElementsByTagName("head")[0].appendChild(s);
}

if(typeof(LDAvis) !== "undefined"){
   // already loaded: just create the visualization
   !function(LDAvis){
       new LDAvis("#" + "ldavis_el5555111629572964624427546", ldavis_el5555111629572964624427546_data);
   }(LDAvis);
}else if(typeof define === "function" && define.amd){
   // require.js is available: use it to load d3/LDAvis
   require.config({paths: {d3: "https://d3js.org/d3.v5"}});
   require(["d3"], function(d3){
      window.d3 = d3;
      LDAvis_load_lib("https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js", function(){
        new LDAvis("#" + "ldavis_el5555111629572964624427546", ldavis_el5555111629572964624427546_data);
      });
    });
}else{
    // require.js not available: dynamically load d3 & LDAvis
    LDAvis_load_lib("https://d3js.org/d3.v5.js", function(){
         LDAvis_load_lib("https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js", function(){
                 new LDAvis("#" + "ldavis_el5555111629572964624427546", ldavis_el5555111629572964624427546_data);
            })
         });
}
</script></div></div>
</div>
<p>LDA with TF-IDF is not giving good results. The intertopic distance for 2 topics is approximately 0.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Get the dominant topic number for each 
def dominant_topics(ldamodel, corpus, tweets, texts):
    sent_topics_df = pd.DataFrame()
    for row in tqdm(ldamodel[corpus]):
        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)
        for j, (topic_num, prop_topic) in enumerate(row):
            if j==0:
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = &quot;, &quot;.join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(
                    pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True
                )
            else:
                break
    sent_topics_df.columns = [&#39;Dominant_Topic&#39;, &#39;Perc_Contribution&#39;, &#39;Topic_Keywords&#39;]
    
    # Add tweets column
    contents = pd.Series(tweets)
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    sent_topics_df.rename(columns={0: &quot;Tweet&quot;}, inplace=True)

    # Add trigrams column
    contents = pd.Series(texts)
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    sent_topics_df.rename(columns={0: &quot;Texts&quot;}, inplace=True)
    return sent_topics_df.copy()

topics_df = dominant_topics(ldamodel=lda_model, 
                            corpus=tfidf_corpus, 
                            tweets=df[&#39;tweet&#39;].values.tolist(), 
                            texts=df[&#39;trigram&#39;].values.tolist())
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████| 2770/2770 [00:02&lt;00:00, 1007.94it/s]
</pre></div>
</div>
</div>
</div>
<p>The dominant topic for each can be given by:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>topics_df.head()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Dominant_Topic</th>
      <th>Perc_Contribution</th>
      <th>Topic_Keywords</th>
      <th>Tweet</th>
      <th>Texts</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0.8728</td>
      <td>like, drink, work, want, good, love, order, da...</td>
      <td>So now you up 3am for work. These boomers 🤬 th...</td>
      <td>[work, boomer, wake, early, bullshxt, stop, bu...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0.8032</td>
      <td>like, drink, work, want, good, love, order, da...</td>
      <td>@ScottPalmer61 Yes. It’s a special Starbucks a...</td>
      <td>[yes, special, attachment]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0.8027</td>
      <td>like, drink, work, want, good, love, order, da...</td>
      <td>I like the caramel frappe from Starbucks https...</td>
      <td>[like, caramel, frappe]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0.8331</td>
      <td>like, drink, work, want, good, love, order, da...</td>
      <td>Why is No Time To Die playing on repeat at #St...</td>
      <td>[time, die, playing, repeat, notice]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0.8521</td>
      <td>like, drink, work, want, good, love, order, da...</td>
      <td>https://t.co/tgR9Z5p8Ts | Criminals steal 200 ...</td>
      <td>[criminal, steal, customer, datum, singapore, ...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The wordcloud for dominant topics is given by:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]

cloud = WordCloud(background_color=&#39;white&#39;,
                  width=2500,
                  height=1800,
                  max_words=15,
                  colormap=&#39;tab10&#39;,
                  color_func=lambda *args, **kwargs: cols[i],
                  prefer_horizontal=1.0)

topics = lda_model.show_topics(formatted=False)

fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)

for i, ax in enumerate(axes.flatten()):
    try:
        fig.add_subplot(ax)
        topic_words = dict(topics[i][1])
        cloud.generate_from_frequencies(topic_words, max_font_size=300)
        plt.gca().imshow(cloud)
        plt.gca().set_title(&#39;Topic &#39; + str(i), fontdict=dict(size=16))
        plt.gca().axis(&#39;off&#39;)
    except:
        pass


plt.subplots_adjust(wspace=0, hspace=0)
plt.axis(&#39;off&#39;)
plt.margins(x=0, y=0)
plt.tight_layout()
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.

/Users/pushkar/miniforge3/envs/project/lib/python3.8/site-packages/wordcloud/wordcloud.py:508: DeprecationWarning:

textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.
</pre></div>
</div>
<img alt="../../_images/lda_44_1.png" src="../../_images/lda_44_1.png" />
</div>
</div>
<p>Though there is an overlap between the second and the third topic, but the topics can be distinguished from each other.</p>
<ol class="simple">
<li><p>The first topic is related to general like of coffee</p></li>
<li><p>The second is related to the company’s products</p></li>
<li><p>The third is related to the feel-good quatient with respect to coffee</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from collections import Counter
topics = lda_model.show_topics(formatted=False)
data_flat = [w for w_list in df[&#39;trigram&#39;].values.tolist() for w in w_list]
counter = Counter(data_flat)

out = []
for i, topic in topics:
    for word, weight in topic:
        out.append([word, i , weight, counter[word]])

df = pd.DataFrame(out, columns=[&#39;word&#39;, &#39;topic_id&#39;, &#39;importance&#39;, &#39;word_count&#39;])        

# Plot Word Count and Weights of Topic Keywords
fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)
cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]
for i, ax in enumerate(axes.flatten()):
    try:
        ax.bar(x=&#39;word&#39;, height=&quot;word_count&quot;, data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label=&#39;Word Count&#39;)
        ax_twin = ax.twinx()
        ax_twin.bar(x=&#39;word&#39;, height=&quot;importance&quot;, data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label=&#39;Weights&#39;)
        ax.set_ylabel(&#39;Word Count&#39;, color=cols[i])
        ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)
        ax.set_title(&#39;Topic: &#39; + str(i), color=cols[i], fontsize=16)
        ax.tick_params(axis=&#39;y&#39;, left=False)
        ax.set_xticklabels(df.loc[df.topic_id==i, &#39;word&#39;], rotation=30, horizontalalignment= &#39;right&#39;)
        ax.legend(loc=&#39;upper left&#39;); ax_twin.legend(loc=&#39;upper right&#39;)
    except:
        pass

fig.tight_layout(w_pad=2)    
fig.suptitle(&#39;Word Count and Importance of Topic Keywords&#39;, fontsize=22, y=1.05)    
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/var/folders/zl/bqtzmz3s0xl5_ddgbjsqxsww0000gn/T/ipykernel_5555/2927128625.py:25: UserWarning:

FixedFormatter should only be used together with FixedLocator

/var/folders/zl/bqtzmz3s0xl5_ddgbjsqxsww0000gn/T/ipykernel_5555/2927128625.py:25: UserWarning:

FixedFormatter should only be used together with FixedLocator

/var/folders/zl/bqtzmz3s0xl5_ddgbjsqxsww0000gn/T/ipykernel_5555/2927128625.py:25: UserWarning:

FixedFormatter should only be used together with FixedLocator
</pre></div>
</div>
<img alt="../../_images/lda_46_1.png" src="../../_images/lda_46_1.png" />
</div>
</div>
<p>Though the dominant words in the two overlapping topics are different. The frequency for words as compared to weights of words is too high in topic 1, while the situation is other way round for topic 2. No inference can be made from such result.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/topic_modelling"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="topic_modeling.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Topic Modeling</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="bertopic.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Topic Modeling: BERTopic</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Pushkar Patil<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>