{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing is an essential step in natural language processing (NLP), which involves cleaning, formatting, and transforming text data to make it suitable for use in NLP algorithms. Some common tasks that are performed as part of data preprocessing in NLP include:\n",
    "- Tokenization: Tokenization is the process of breaking a piece of text into individual words or tokens. This is typically done by splitting the text on whitespace and punctuation, and then applying additional rules to identify and separate words that are joined together (such as contractions or compound nouns).\n",
    "- Removing stop words: Stop words are common words that are typically filtered out of NLP algorithms because they do not add significant meaning to the text (such as \"the,\" \"and,\" or \"but\"). Removing stop words can help to reduce the size of the dataset and can make it easier to identify the most important words and phrases in the text.\n",
    "- Stemming and lemmatization: Stemming and lemmatization are techniques for reducing inflected words (such as verbs in different tenses) to their base form. This can help to group together words that have the same meaning, which can improve the performance of NLP algorithms.\n",
    "- Encoding the text: NLP algorithms typically require that text be encoded as numbers rather than as raw text. This can be done using techniques such as one-hot encoding, which creates a binary feature for each possible word in the vocabulary, or using word embeddings, which represent each word as a dense vector of numbers.\n",
    "\n",
    "Data preprocessing can help to improve the performance and accuracy of NLP algorithms by making the text data more suitable for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required packages\n",
    "import re\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import spacy\n",
    "import swifter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "tqdm.pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the time of processing, it will be prudent to analyze only a subset of tweets. Thus, for the further analysis 4000 tweets will be selected at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and select a subset of 4000 observations\n",
    "df = pd.read_hdf('./../../code/data/starbucks/data.h5', key='starbucks')\n",
    "df = df.sample(n=4000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm the shape of the data\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the observation from the exploratory data analysis, the following prelimnary preprocessing steps will be performed:\n",
    "1. Remove the duplicate tweets\n",
    "2. Remove blank tweets data\n",
    "3. Select the subset of data which are posted in the English language\n",
    "4. Remove tweet data from official company handles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df.drop_duplicates('tweet', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove blank tweets data\n",
    "df.dropna(subset='tweet', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing duplicate and blank tweets, 3658 number of tweets still exist in the data.\n"
     ]
    }
   ],
   "source": [
    "print(f\"After removing duplicate and blank tweets, {df.shape[0]} number of tweets still exist in the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select observations posted in the English language\n",
    "df = df[df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data from official company handles\n",
    "df = df[~df['username'].isin(['starbucks_es', 'starbucks_cstm', 'starbucks_j_cpg', 'starbuckspoho', \n",
    "                                'starbuckspoho', 'starbuckshomeme', 'starbucksperu'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing duplicate, blank tweets, selecting subset based on language, removing data from official handles - 2770 number of tweets remian in the data.\n"
     ]
    }
   ],
   "source": [
    "print(f\"After removing duplicate, blank tweets, selecting subset based on language, removing data from official handles - {df.shape[0]} number of tweets remian in the data.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying any algorithm to the data, it is necessary clean the data. The preprocessing steps that needs to be performed to make the tweet data usable are as follows:\n",
    "1. Convert the tweets to lower case\n",
    "2. Remove any tagged users\n",
    "3. Remove hashtags from the tweets\n",
    "4. Remove links from the tweets\n",
    "5. Remove any special characters and retain only alphanumeric data\n",
    "6. Perform lemmatization\n",
    "7. Remove the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(text):\n",
    "    \"\"\"Method to perform preprocessing of tweets\"\"\"\n",
    "    text = str(text).lower() # Convert tweets to lower case\n",
    "    text = re.sub(r'@ *\\w*', '', str(text)) # Remove tagged usernames from the tweets\n",
    "    text = re.sub(r'#\\w+', '', str(text)) # Remove hashtags from the tweets\n",
    "    text = re.sub('\\n', ' ', str(text)) # Remove newline characters from the tweets\n",
    "    text = re.sub('\\xa0', ' ', str(text)) # Remove special characters coverted to strings from the tweets. In this case it is \"\\xa0\"\n",
    "    text = re.sub('&amp', ' ', str(text)) # Remove \"&amp\" from the tweets\n",
    "    text = re.sub(r'http\\S+', '', str(text)) # Remove links from the tweets\n",
    "    text = re.sub(r'[^A-Za-z0-9 ]+', '', str(text)) # Keep only alphanumeric characters in the tweets\n",
    "    return text\n",
    "\n",
    "df.loc[:, 'preprocessed_tweet'] = df['tweet'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove blank tweets after preprocessing\n",
    "df = df[df['preprocessed_tweet'].map(bool)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a technique used in natural language processing (NLP) to reduce inflected words to their base form, or lemma. This is done by identifying the part of speech and the context of the word, and then applying rules to map it to its lemma. For example, the lemma of the word \"was\" is \"be,\" and the lemma of the word \"better\" is \"good.\" Lemmatization is similar to stemming, which also reduces words to their base form, but lemmatization is more accurate because it considers the context of the word and produces a valid lemma, whereas stemming simply removes suffixes from the word, which may not result in a valid word. Lemmatization can be useful for grouping together words that have the same meaning, which can improve the performance of NLP algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151d289f4a63426885fb7e124b5cc60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2770 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform lemmatization\n",
    "def lemmatization(text):\n",
    "    \"\"\"Function to lemmatized tokenied sentence\"\"\"\n",
    "    return \" \".join([token.lemma_ if token.lemma_ != '-PRON-' else token.lower_ for token in nlp(text)])\n",
    "\n",
    "df['preprocessed_tweet'] = df['preprocessed_tweet'].swifter.apply(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b491514921d940a08feac0fee1aae1a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2770 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Method to remove stopwords from tweet text\"\"\"\n",
    "\n",
    "    stopwords = STOPWORDS.union(set(['starbucks', 'starbuck']))\n",
    "\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stopwords]\n",
    "    return \" \".join(tokens_without_sw)\n",
    "\n",
    "df['preprocessed_tweet'] = df['preprocessed_tweet'].swifter.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>preprocessed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>477970</th>\n",
       "      <td>So now you up 3am for work. These boomers 🤬 th...</td>\n",
       "      <td>3 a.m. work boomer wake early bullshxt stop bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362474</th>\n",
       "      <td>@ScottPalmer61 Yes. It’s a special Starbucks a...</td>\n",
       "      <td>yes special attachment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312427</th>\n",
       "      <td>I like the caramel frappe from Starbucks https...</td>\n",
       "      <td>like caramel frappe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71318</th>\n",
       "      <td>Why is No Time To Die playing on repeat at #St...</td>\n",
       "      <td>time die playing repeat notice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412468</th>\n",
       "      <td>https://t.co/tgR9Z5p8Ts | Criminals steal 200 ...</td>\n",
       "      <td>criminal steal 200 000 customer datum singapor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38551</th>\n",
       "      <td>@ShevyShevrolet @HAWTToys @McFaul @elonmusk On...</td>\n",
       "      <td>month solve profs dilemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441914</th>\n",
       "      <td>If @Starbucks and @Delta are doing this I real...</td>\n",
       "      <td>hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744590</th>\n",
       "      <td>#Starbucks is taking its first steps into the ...</td>\n",
       "      <td>step partnership build blockchainbase loyalty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31846</th>\n",
       "      <td>@notany1youllkno @THETOMMYDREAMER @Starbucks @...</td>\n",
       "      <td>feel ok sure maga trump proud rock maga brother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393878</th>\n",
       "      <td>Fidelity and Starbucks team up to help employe...</td>\n",
       "      <td>fidelity team help employee build saving</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2770 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    tweet  \\\n",
       "477970  So now you up 3am for work. These boomers 🤬 th...   \n",
       "362474  @ScottPalmer61 Yes. It’s a special Starbucks a...   \n",
       "312427  I like the caramel frappe from Starbucks https...   \n",
       "71318   Why is No Time To Die playing on repeat at #St...   \n",
       "412468  https://t.co/tgR9Z5p8Ts | Criminals steal 200 ...   \n",
       "...                                                   ...   \n",
       "38551   @ShevyShevrolet @HAWTToys @McFaul @elonmusk On...   \n",
       "441914  If @Starbucks and @Delta are doing this I real...   \n",
       "744590  #Starbucks is taking its first steps into the ...   \n",
       "31846   @notany1youllkno @THETOMMYDREAMER @Starbucks @...   \n",
       "393878  Fidelity and Starbucks team up to help employe...   \n",
       "\n",
       "                                       preprocessed_tweet  \n",
       "477970  3 a.m. work boomer wake early bullshxt stop bu...  \n",
       "362474                             yes special attachment  \n",
       "312427                                like caramel frappe  \n",
       "71318                      time die playing repeat notice  \n",
       "412468  criminal steal 200 000 customer datum singapor...  \n",
       "...                                                   ...  \n",
       "38551                           month solve profs dilemma  \n",
       "441914                                               hope  \n",
       "744590  step partnership build blockchainbase loyalty ...  \n",
       "31846     feel ok sure maga trump proud rock maga brother  \n",
       "393878           fidelity team help employee build saving  \n",
       "\n",
       "[2770 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the sample data after performing all the preprocessing steps\n",
    "df[['tweet', 'preprocessed_tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zl/bqtzmz3s0xl5_ddgbjsqxsww0000gn/T/ipykernel_83633/609922302.py:1: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block1_values] [items->Index(['Unnamed: 0', 'tweet', 'conversation_id', 'date', 'hashtags',\n",
      "       'inReplyToTweetId', 'reply_to', 'language', 'likes_count', 'media',\n",
      "       'mentions', 'quoted_tweet', 'retweets_count', 'link',\n",
      "       'user_status_count', 'location', 'name', 'description', 'verified',\n",
      "       'url', 'user_id', 'username', 'preprocessed_tweet'],\n",
      "      dtype='object')]\n",
      "\n",
      "  df.to_hdf('./../../code/data/starbucks/data.h5', key='preprocessed_starbucks')\n"
     ]
    }
   ],
   "source": [
    "# Save the preprocessed data for further use\n",
    "df.to_hdf('./../../code/data/starbucks/data.h5', key='preprocessed_starbucks')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af77eb27f514ee114388f6898d9553454263f7eea260918546d16a8e581c8922"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
